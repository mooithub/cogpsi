# Reinforcement Learning

## MDP Part 2 : Gridworld

### 2019. 7. 15. Mon

### CogPsi Study

### YK LEE

---

## 1. 그리드월드와 MDP

### 1.1. 그리드월드와 MDP 구성 요소

그리드월드의 상태가 5개 잇다면, 수식 2.1과 같이 표현 가능

#### 수식 2.1

#### _S_ = {(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),(x<sub>3</sub>,y<sub>3</sub>),(x<sub>4</sub>,y<sub>4</sub>),(x<sub>5</sub>,y<sub>5</sub>)}

####그림 2.2
그리드 월드에서 상태는 좌표를 의미한다.

그림 2.2와 같이 그리드월드에서는 격자 상의 각 위치(좌표) = **상태**
여기서 상태는 총 25개 (=격자의 위치)

에이젼트 = 빨간색 네모

그림 2.2처럼 에이젼트가 (1,1) 위치에 있으면 에이젼트의 상태는 (1,1)이 된다.

그리드월드의 상태를 집합으로 나타낸다면 수식 2.2와 같다.

#### 수식 2.2

그리드월드의 상태 집합

#### S = {(1,1),(1,2),(1,3)...,(5,5)}

각 상태는 (x,y)로 이뤄진 좌표로서 그리드월드의 가로축이 x축이고 세로축이 y축이다. Agent는 시간(t)에 따라 25개의 상태의 집합(S) 안에 있는 상태(S<sub>t</sub>)를 탐험한다.

#### 수식 2.3.

시간 t일 때 상태가 (1,3)일 경우의 수식 표현

#### S<sub>t</sub> = (1,3)

### 1.2. 확률 변수

각 시간t에 대한 상태 S<sub>t</sub>는 정해져있지 않다. t = 1여도 상태는 S<sub>t</sub> = (1,3)일 수도 있고, (4,2)일 수도 있다. = 집합 안에서 뽑을 때마다 달라질 수 있는 것을 **확률 변수**라고 한다.

확률 변수의 예: 주사위
주사위는 던질 때 마다 나오는 값이 달라진다.

확률 변수는 대문자로 표현한다. 따라서 시간 t에서의 상태를 S<sub>t</sub>와 같이 대문자로 쓴다.

#### 수식 2.4.

#### S<sub>t</sub> = s

시간 t 에서의 상태 S<sub>t</sub>가 어떤 상태 s 일 때.

### 1.3. 행동

A = 에이젼트가 상태 S<sub>t</sub>에서 할 수 있는 가능한 행동의 집합

a = 어떤 특정한 행동 (어떤 특정한 상태가 s, 소문자이듯.)

보통 에이젼트가 할 수 있는 행동은 모든 상태에서 같기 때문에 하나의 집합 A로 나타낼 수 있다.

#### 수식 2.5

#### A<sub>t</sub> = a

시간 t에서의 행동 a

A<sub>t</sub>는 어떤 t라는 시간에 집합 A에서 선택한 행동이다. t라는 시간에 에이젼트가 어떤 행동을 할지 정해져 있는 것이 아니므로 A<sub>t</sub>는 확률변수이고 대문자로 표현한다.

보통 에이젼트가 할 수 있는 행동들의 집합은 한 문제 내에서 변하지 않는다. 그리드월드에서 에이젼트가 할 수 있는 행동은 위, 아래, 왼쪽, 오른쪽으로 움직이기로 총 네 가지이다.

#### 수식 2.6

#### A = {up, down, left, right}

그리드월드에서 에이젼트가 할 수 있는 행동의 집합.

만약 시간t에서 상태가 (3,1)이고 A<sub>t</sub> = right라면 다음 시간의 상태는 그림 2.4와 같이 (4,1)이 된다.

#### 그림 2.4 그리드월드 에이젼트 이동

<img src = "./pic2_4.png" width = 100/>

어떤 상태에서 행동을 한 후 에이젼트가 이동한다.

---

### 1.4. 보상함수

보상 = reward = 에이젼트가 학습할 수 있는 정보 (scalar number)

#### 수식 2.7. 보상함수<sub>Reward Function</sub>의 정의

R<sup>a</sup><sub>s</sub> = E[R<sub>t+1</sub> | S<sub>t</sub> = s, A<sub>t</sub> = a]

시간t에서 상태 S<sub>t</sub> = s 이고 행동이 A<sub>t</sub> = a 일때 에이젼트가 받을 보상의 기댓값(Expectation, E)

기댓값(Expectation) = 일종의 평균. 나오게 될 값의 예상. 여기서는 상태 s 에서 행동 a 를 했을 경우에 받을 것이라 예상되는 숫자임. 수식 2.9에서처럼 대문자 E로 표시.

에이젼트에게 보상을 주는 것은 환경이고, 환경에 따라서 같은 상태에서 같은 행동을 취하더라도 다른 보상을 줄 수도 있기 때문에, 이 모든 걸 고려하여 보상 함수를 기댓값으로 표현한다.

t+1 = 한 단계 이후의 상태. 보상을 받는 시점.

타임 스텝<sub>time step</sub>

### 1.5. 상태 변환 확률

#### 수식 2.12. 상태 변환 확률

P<sup>a</sup><sub>SS'</sub> = P[S<sub>t+1</sub> = s' | S<sub>t</sub> = s, A<sub>t</sub> = a]

상태s에서 행동a를 취했을 때 다른 상태 s'에 도달할 확률.

보상과 마찬가지로 에이젼트가 알지 못하는 값으로서 **에이젼트가 아닌, 환경의 일부**이다.

환경은 에이젼트가 행동을 취하면 상태 변환 확률을 통해 다음에 에이젼트가 갈 상태를 알려준다.

### 1.6. 감가율

#### 수식 2.13 감가율의 정의

#### &gamma; &isin; [0,1]

#### 수식 2.14 감가율을 고려한 미해 보상의 현재 가치

### &gamma;<sup>k-1</sup>R<sub>t+k</sub>

### 1.7. 정책

#### 수식 2.15. 정책의 정의

#### &pi;(a|s) = P[A<sub>t</sub> = a| S<sub>t</sub> = s]

시간 t에 S<sub>t</sub> = s에 에이젼트가 있을 때 가능한 행동 중에서 A<sub>t</sub>= a 를 할 확률.

---

### 2. 가치함수

### 2.1. 가치함수 정의

에이젼트가 학습할 수 있도록 문제를 MDP로 정의했으므로 최적의 정책(policy)을 찾으면 된다. 에이젼트 입장에서 어떤 행동이 좋은지를 알 수 있게, 어떠한 상태에서 기대하는 보상(값)을 가치 함수라고 한다 (**Reward Function**)

보상은 행동을 했을 때가 아닌 그다음 **타임 스텝**에 받는다. 따라서 시간 t에 행동을 해서 받는 보상은 R<sub>t+1</sub>이다. 그다음 시간t에 받는 보상은 R<sub>t+2</sub>.. 이렇게 각 시간마다 받는 보상을 다 합하면 수식 2.16와 같은 식이 나온다.

#### 수식 2.16. 일련의 보상들의 단순합

#### R<sub>t+1</sub> + R<sub>t+2</sub> + R<sub>t+3</sub> + R<sub>t+4</sub> + R<sub>t+5</sub> + ...

하지만 위의 식 2.6처럼 보상을 감가하지 않고 더하면 문제가 발생 (3가지)

1. 에이젼트 입장에서는 현재 보상 = 미래 보상으로 취급하게 됨. 지금 받는 보상 100과 이후 타임 스텝에서 받을 보상 100의 차이가 없음

2. 100번이라는 보상을 한 번 받는 것과 20이라는 보상 5번 받는 것을 구분할 수 없음 (_사람에겐 쉬우나 에이젼트는 불가능, 조삼모사 vs 7_)

3. 시간이 무한대라고 하면 보상을 시간마다 0.1씩 받아도 합이 무한대이고, 1씩 받아도 무한대인 합이 나오니 수치적으로 두 경우를 구분할 수 없음 (보상이 0.1인 액션 vs 1인 액션)

따라서 어느정도 패널티인 감가율을 이용한다.

#### 그림 2.13. 감가율을 고려한 미래 보상의 현재 가치

#### 수식 2.18. 감가율을 적용한 보상들의 합

#### R<sub>t+1</sub> + &gamma;R<sub>t+2</sub> + &gamma;<sup>2</sup>R<sub>t+3</sub> + &gamma;<sup>3</sup>R<sub>t+4</sub> + ...

수식 2.18에 나타난 합을 반환값<sup>Return</sup>G<sub>t</sub>라고 한다. 다시 G<sub>t</sub>에 대해 정리해보면 수식 2.19와 같다.

#### 수식 2.19. 반환값(G<sub>t</sub>)의 정의

G<sub>t</sub> = R<sub>t+1</sub> + &gamma;R<sub>t+2</sub>+ &gamma;<sup>2</sup>R<sub>t+3</sub> ...

\*\*반환값(G<sub>t</sub>)은 에이젼트가 실제로 환경을 탐험하며 받은 보상의 합이다. 에이젼트가 에피소드가 끝난 후에 '그때 이후로 (총) 얼마의 보상을 얻은 거지?"라며 보상들을 정산하는 것이 반환값이다.

만일 에피소드를 t = 1부터 5까지 진행했다면 에피소드가 끝난 후에 방문했던 상태들에 대한 5개의 반환값이 생긴다.

#### 수식 2.20. 받은 보상의 정산: 반환값.

G<sub>1</sub> = R<sub>2</sub> + &gamma;R<sub>3</sub> + &gamma;<sup>2</sup>R<sub>4</sub> + &gamma;<sup>3</sup>R<sub>5</sub> + &gamma;<sup>4</sup>R<sub>6</sub>

G<sub>2</sub> = R<sub>3</sub> + &gamma;R<sub>4</sub> + &gamma;<sup>2</sup>R<sub>5</sub> + &gamma;<sup>3</sup>R<sub>6</sub>

G<sub>3</sub> = R<sub>4</sub> + &gamma;R<sub>5</sub> + &gamma;<sup>2</sup>R<sub>6</sub>

G<sub>4</sub> = R<sub>5</sub> + &gamma;R<sub>6</sub>

G<sub>5</sub> = R<sub>6</sub>

에이젼트는 반환값을 에피소드가 끝난 후에야 알 수 있다. 하지만 꼭 에피소드가 끝날때까지 기다리지 않아도 (정확하진 않지만) 현재의 정보를 토대로 행동하는 것이 나을 때가 있다. 앞으로 펼쳐질 환경을 모두 경험하지 않아도, 즉, 가능한 환경에 대해 모두 알고 있지 않아도 (대략) **얼마의 보상을 받을 것이라고 예측 (=기댓값 계산)** 할 수 있다. => **가치함수**

#### 수식 2.21. 가치함수

#### v(s) = E[G<sub>t</sub> | S<sub>t</sub> = s]

- 각 타임스텝마다 받는 보상이 모두 확률적
- 반환값이 그 보상들의 합이므로 반환값 = 확률변수
- 반면, 가치함수는 확률변수가 아니라 **특정 양**을 타나내는 (scalar)값이므로 '소문자'로 표현한다.
- 가치함수 = 에이젼트가 가지고 있는 값.
- 현재 에이젼트가 갈 수 있는 상태들의 가치를 안다면 그중에서 가장 가치가 제일 높은 상태를 선택 가능

#### 수식 2.22. 앞으로 받을 보상에 대한 기댓값인 가치함수

v(s) = E[R<sub>t+1</sub> + &gamma;R<sub>t+2</sub> + &gamma;<sup>2</sup>R<sub>t+3</sub> ... | S<sub>t</sub> = s]

&gamma;R<sub>t+2</sub> 부터 뒤의 항을 &gamma;로 묶어주고 그것을 반환값의 형태로 표현한다.

#### 수식 2.23. 반환값으로 나타내는 가치함수

v(s) = E[R<sub>t+1</sub> + &gamma;(R<sub>t+2</sub> + &gamma;R<sub>t+3</sub>...)| S<sub>t</sub> = s]

v(s) = E[R<sub>t+1</sub> + &gamma;G<sub>t+1</sub> | S<sub>t</sub> = s]

R<sub>t+2</sub> + &gamma;R<sub>t+3</sub> .. 부분을 반환값의 형태로 표현하긴 했으나 에이젼트가 **실제로** 받는 보상은 아니며, **앞으로 받을 것이라 예상하는 보상**이다. 따라서 이 부분을 앞으로 받을 보상에 대한 기댓값인 가치함수로 표현할 수 있다.(수식 2.24)

#### 수식 2.24. 가치함수로 표현하는 가치함수의 정의

v(s) = E[R<sub>t+1</sub> + &gamma;v(S<sub>t+1</sub>) | S<sub>t</sub> = s]

여기까지는 가치함수를 정의할 때 정책을 고려하지 않는다. 하지만 에이젼트가 앞으로 받을 보상에 대해 생각할 때 정책은 중요하다. 상태에서 다음 상태(S<sub>t</sub>-> S<sub>t+1</sub>)로 넘어갈 때 에이젼트는 무조건 행동을 해야하고 각 상태에서 행동을 하는 것이 에이젼트의 정책이기 때문이다. 정책에 따라서 계산하는 가치 함수는 (당연히) 달라질 수 밖에 없다.

즉, MDP에서 가치함수는 항상 **정책**을 고려해야한다. 가치함수에서 정책을 고려한다면 아래 수식과 같이 표현할 수 있다. (가치함수와 기댓값의 기호 밑에 정책을 씀 = 정책 고려한 가치함수)

#### 수식 2.25. 정책을 고려한 가치함수의 표현

#### v<sub>&pi;</sub>(s) = E<sub>&pi;[R<sub>t+1</sub> + &gamma;v<sub>&pi;</sub>(S<sub>t+1</sub>)|S<sub>t</sub> = s]

수식 2.25는 강화학습에서 **상당히** 중요한 벨만 기대 방정식<sup>Bellman Expectation Equation</sup>이다. 벨만 기대 방정식은 **현재 상태의 가치함수(v<sub>&pi;</sub>(S<sub>t+1</sub>))사이의 관계**를 말해주는 방정식이다. 강화학습은 벨만 방정시글 어떻게 풀어가느냐의 스토리이다.
